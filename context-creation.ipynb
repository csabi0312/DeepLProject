{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6146260,"sourceType":"datasetVersion","datasetId":3521629}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/marcellemmer/context-creation?scriptVersionId=153416384\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Context creation using Wikipedia\n\nWe downloaded the 13GB Wikipedia Plaintext (2023-07-01) dataset from Kaggle. This dataset contains all the wikipedia articles up to the mentioned date stored in parquet files. We use only the wiki_2023_index.parquet file that contains the first sentences of the articles as context for our model. We assing a context column for each question of the Q&A dataframe from this file. This is done with the Sentence Transformer library that embeds the wikipedia articles and with Faiss that does a similarity search between the question and the first sentences of the articles. We retrieve the most similar wikipedia article for each question.\n\nThe code uses 2xT4 GPU from Kaggle\n\n## Sources\n\n* https://www.kaggle.com/datasets/jjinho/wikipedia-20230701/data?select=h.parquet\n\n* https://github.com/facebookresearch/faiss/wiki","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install datasets\n!pip install faiss-gpu sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:28:07.935496Z","iopub.execute_input":"2023-12-01T21:28:07.935794Z","iopub.status.idle":"2023-12-01T21:28:38.353458Z","shell.execute_reply.started":"2023-12-01T21:28:07.935767Z","shell.execute_reply":"2023-12-01T21:28:38.352373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the libraries\nimport os\nimport pandas as pd\nfrom datasets import load_dataset\nimport faiss\nfrom sentence_transformers import SentenceTransformer\nimport torch\n\n# Important parameters describing the code\nSIM_MODEL = 'all-MiniLM-L6-v2'\n# Set device\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n#Loading the questions\nqna_df = pd.read_csv(\"https://raw.githubusercontent.com/csabi0312/DeepLProject/main/train.csv\",index_col=0)\n\nqna_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:28:38.355549Z","iopub.execute_input":"2023-12-01T21:28:38.35589Z","iopub.status.idle":"2023-12-01T21:28:46.046082Z","shell.execute_reply.started":"2023-12-01T21:28:38.35586Z","shell.execute_reply":"2023-12-01T21:28:46.045157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Parquet files into a Hugging Face dataset\n# Source: https://www.kaggle.com/datasets/jjinho/wikipedia-20230701/data?select=wiki_2023_index.parquet\nwiki_dataset = load_dataset('parquet', data_files={'train': \"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\"}, split='train') # 1.76GB file","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:28:46.047325Z","iopub.execute_input":"2023-12-01T21:28:46.047635Z","iopub.status.idle":"2023-12-01T21:29:18.892735Z","shell.execute_reply.started":"2023-12-01T21:28:46.04759Z","shell.execute_reply":"2023-12-01T21:29:18.891453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained sentence transformer model\nmodel = SentenceTransformer(SIM_MODEL)\n\n# Create a Faiss index\nindex = faiss.IndexFlatIP(model.get_sentence_embedding_dimension())\n\n# Define batch size\nbatch_size = 500_000\n\n# Iterate over the dataset in batches\nfor i in range(0, len(wiki_dataset['context']), batch_size):\n    # Encode the context sentences using the SentenceTransformer model\n    context_embeddings = model.encode(wiki_dataset['context'][i:i+batch_size],\n                                      device=DEVICE,\n                                      show_progress_bar=True,\n                                      convert_to_tensor=True,\n                                      normalize_embeddings=True).half()  # Use mixed-precision training (FP16) to reduce memory footprint\n\n    # Convert the embeddings to a numpy array\n    context_embeddings_np = context_embeddings.detach().cpu().numpy()\n    context_embeddings_np = context_embeddings_np.astype('float32')\n\n    # Add the embeddings to the Faiss index\n    index.add(context_embeddings_np)\n\n    # Free up memory\n    del context_embeddings, context_embeddings_np\n\n# Function to retrieve most similar documents\ndef retrieve_most_similar(query, k=20):\n    query_embedding = model.encode(query, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n    query_embedding = query_embedding.reshape(1, -1)  # Reshape for Faiss\n    query_embedding = query_embedding.detach().cpu().numpy()\n    _, idx = index.search(query_embedding, k)\n    return idx[0]\n\n# Example usage\nquery_text = qna_df['prompt'][0]\nprint(f'example prompt {query_text}')\nsimilar_documents_indices = retrieve_most_similar(query_text)\n\n# Print similar documents\nfor idx in similar_documents_indices:\n    print(wiki_dataset[int(idx)]['context'])","metadata":{"execution":{"iopub.status.busy":"2023-12-01T21:29:18.894838Z","iopub.execute_input":"2023-12-01T21:29:18.895169Z","iopub.status.idle":"2023-12-01T22:29:47.96633Z","shell.execute_reply.started":"2023-12-01T21:29:18.895139Z","shell.execute_reply":"2023-12-01T22:29:47.965303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the context column from the wikipedia article\n# Create an empty list to store the context for each prompt\ncontext_list = []\n\n# Loop through each prompt in the qna_df dataframe\nfor i in range(len(qna_df)):\n    query_text = qna_df['prompt'][i]\n    similar_documents_indices = retrieve_most_similar(query_text)\n\n    # Get the first answer from the corresponding wiki_dataset\n    context = wiki_dataset[int(similar_documents_indices[0])]['context']\n\n\n    context_list.append(context)\n\n# Add the context_list as a new column \"context\" to the qna_df DataFrame\nqna_df['context'] = context_list\n\n# Save the Q&A DataFrame to a CSV file\nqna_df.to_csv('openbook-qna-data.csv', index=False)\n\n# Display the modified DataFrame\nqna_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-01T22:29:47.967764Z","iopub.execute_input":"2023-12-01T22:29:47.968076Z","iopub.status.idle":"2023-12-01T22:40:04.67568Z","shell.execute_reply.started":"2023-12-01T22:29:47.968047Z","shell.execute_reply":"2023-12-01T22:40:04.674689Z"},"trusted":true},"execution_count":null,"outputs":[]}]}