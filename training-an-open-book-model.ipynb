{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How To Train Model for Open Book Q&A Technique\nIn this notebook we demonstrate how to train a model to be used with top scoring Open Book Q&A method. The Open Book method was first presented by JJ (@jjinho) [here][1], then Quangteo (@quangbk) improved RAM usage [here][2], and Anil (@nlztrk) combined with Q&A [here][3]. Radek (@radek1) demonstrated the strength of Q&A [here][5]. Next Mgoksu (@mgoksu) demonstrated how to achieve top public LB=0.807 using this method [here][4] by finetuning DeBerta large on this method.\n\nIn order to train a model for use with Open Book Q&A, we need a CSV that contains; `prompt` (i.e. question), `A, B, C, D, E` (i.e. answer choices), and we need a column of `context` extracted from wikipedia pages for each question. To generate the `context` column, we run Mgoksu's notebook [here][4]. In code cell #5, we load our CSV without `context` column with code `trn = pd.read_csv(OUR_DATASET.CSV)`. Then in code cell #21 our dataset is saved to disk as `test_context.csv` with the column `context` added.\n\nI have searched and concatenated all publicly shared datasets into one 60k CSV and then ran Mgoksu's notebook with `NUM_TITLES_INCLUDE = 5` and `NUM_SENTENCES_INCLUDE = 20`. This added an additional `context` column. I uploaded the resultant CSV file to a Kaggle dataset [here][6]. If you enjoy the notebook you are reading, please upvote the dataset too. Thanks! \n\n![](https://miro.medium.com/v2/resize:fit:800/format:webp/1*bTGY3fKIgNefQxNsOYpnBw.png)\n \n(image source [here][7])\n\n[1]: https://www.kaggle.com/code/jjinho/open-book-llm-science-exam\n[2]: https://www.kaggle.com/code/quangbk/open-book-llm-science-exam-reduced-ram-usage\n[3]: https://www.kaggle.com/code/nlztrk/openbook-debertav3-large-baseline-single-model\n[4]: https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model\n[5]: https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training\n[6]: https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2\n[7]: https://blog.gopenai.com/enrich-llms-with-retrieval-augmented-generation-rag-17b82a96b6f0","metadata":{}},{"cell_type":"markdown","source":"# Load CSV\nWe will load 60k CSV of `prompts`, `A,B,C,D,E`, and `context` from my Kaggle dataset [here][1]. This dataset is all publicly shared datasets concatenated then processed with Mgoksu's notebook [here][2] to create a `context` column. (To learn more about the datasets within read my discussion post). This Kaggle dataset also contains competition `train.csv` with added `context` column (to be used as a validation dataset).\n\nIn this train notebook, we have internet turned on and can choose whatever model we wish to download and train. After we finetune this model, we will create a second notebook with the Open Book Q&A technique and load the finetuned model from the output of this notebook. The second notebook will have internet turned off so that it can be submitted to Kaggle's competition.\n\n[1]: https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2\n[2]: https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model","metadata":{}},{"cell_type":"code","source":"import os\n# Set CUDA visible devices to GPU 0 and 1 (The 2xT4 GPUs that Kaggle provides)\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n\nimport os\nfrom typing import Optional, Union\nimport pandas as pd, numpy as np, torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom datasets import Dataset, load_metric\nfrom dataclasses import dataclass\nfrom transformers import AutoTokenizer, AutoModelForMultipleChoice, EarlyStoppingCallback, \\\n                         TrainingArguments, Trainer, set_seed\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n\n# Random seed\nseed = 42\nset_seed(seed)\n# Define constants\nVER=1\n# Number of layers to freeze, DeBERTa has a total number of 24 layers\nFREEZE_LAYERS = 18\n# Boolean to freeze embeddings\nFREEZE_EMBEDDINGS = True\n# Length of context + question + answers\nMAX_INPUT = 256\n# The Hugging Face model we're using\nMODEL = 'microsoft/deberta-v3-large'","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:17:26.623643Z","iopub.execute_input":"2023-12-04T12:17:26.624620Z","iopub.status.idle":"2023-12-04T12:17:26.631814Z","shell.execute_reply.started":"2023-12-04T12:17:26.624583Z","shell.execute_reply":"2023-12-04T12:17:26.630886Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# Read validation data from a CSV file\nqna_df = pd.read_csv('/kaggle/input/openbook-qna/openbook-qna-data.csv')\nprint('Validation data size:', qna_df.shape )\nqna_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:17:26.682497Z","iopub.execute_input":"2023-12-04T12:17:26.682802Z","iopub.status.idle":"2023-12-04T12:17:26.707418Z","shell.execute_reply.started":"2023-12-04T12:17:26.682776Z","shell.execute_reply":"2023-12-04T12:17:26.706486Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Validation data size: (200, 8)\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  Which of the following statements accurately d...   \n1  Which of the following is an accurate definiti...   \n2  Which of the following statements accurately d...   \n3  What is the significance of regularization in ...   \n4  Which of the following statements accurately d...   \n\n                                                   A  \\\n0  MOND is a theory that reduces the observed mis...   \n1  Dynamic scaling refers to the evolution of sel...   \n2  The triskeles symbol was reconstructed as a fe...   \n3  Regularizing the mass-energy of an electron wi...   \n4  The angular spacing of features in the diffrac...   \n\n                                                   B  \\\n0  MOND is a theory that increases the discrepanc...   \n1  Dynamic scaling refers to the non-evolution of...   \n2  The triskeles symbol is a representation of th...   \n3  Regularizing the mass-energy of an electron wi...   \n4  The angular spacing of features in the diffrac...   \n\n                                                   C  \\\n0  MOND is a theory that explains the missing bar...   \n1  Dynamic scaling refers to the evolution of sel...   \n2  The triskeles symbol is a representation of a ...   \n3  Regularizing the mass-energy of an electron wi...   \n4  The angular spacing of features in the diffrac...   \n\n                                                   D  \\\n0  MOND is a theory that reduces the discrepancy ...   \n1  Dynamic scaling refers to the non-evolution of...   \n2  The triskeles symbol represents three interloc...   \n3  Regularizing the mass-energy of an electron wi...   \n4  The angular spacing of features in the diffrac...   \n\n                                                   E answer  \\\n0  MOND is a theory that eliminates the observed ...      D   \n1  Dynamic scaling refers to the evolution of sel...      A   \n2  The triskeles symbol is a representation of th...      A   \n3  Regularizing the mass-energy of an electron wi...      C   \n4  The angular spacing of features in the diffrac...      D   \n\n                                             context  \n0  In cosmology, the missing baryon problem is an...  \n1  Dynamic scaling (sometimes known as Family-Vic...  \n2  thumb|Neolithic triple spiral symbol A triskel...  \n3  In physics, especially quantum field theory, r...  \n4  Kinematic diffraction is the approach to study...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n      <th>E</th>\n      <th>answer</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Which of the following statements accurately d...</td>\n      <td>MOND is a theory that reduces the observed mis...</td>\n      <td>MOND is a theory that increases the discrepanc...</td>\n      <td>MOND is a theory that explains the missing bar...</td>\n      <td>MOND is a theory that reduces the discrepancy ...</td>\n      <td>MOND is a theory that eliminates the observed ...</td>\n      <td>D</td>\n      <td>In cosmology, the missing baryon problem is an...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Which of the following is an accurate definiti...</td>\n      <td>Dynamic scaling refers to the evolution of sel...</td>\n      <td>Dynamic scaling refers to the non-evolution of...</td>\n      <td>Dynamic scaling refers to the evolution of sel...</td>\n      <td>Dynamic scaling refers to the non-evolution of...</td>\n      <td>Dynamic scaling refers to the evolution of sel...</td>\n      <td>A</td>\n      <td>Dynamic scaling (sometimes known as Family-Vic...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Which of the following statements accurately d...</td>\n      <td>The triskeles symbol was reconstructed as a fe...</td>\n      <td>The triskeles symbol is a representation of th...</td>\n      <td>The triskeles symbol is a representation of a ...</td>\n      <td>The triskeles symbol represents three interloc...</td>\n      <td>The triskeles symbol is a representation of th...</td>\n      <td>A</td>\n      <td>thumb|Neolithic triple spiral symbol A triskel...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What is the significance of regularization in ...</td>\n      <td>Regularizing the mass-energy of an electron wi...</td>\n      <td>Regularizing the mass-energy of an electron wi...</td>\n      <td>Regularizing the mass-energy of an electron wi...</td>\n      <td>Regularizing the mass-energy of an electron wi...</td>\n      <td>Regularizing the mass-energy of an electron wi...</td>\n      <td>C</td>\n      <td>In physics, especially quantum field theory, r...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Which of the following statements accurately d...</td>\n      <td>The angular spacing of features in the diffrac...</td>\n      <td>The angular spacing of features in the diffrac...</td>\n      <td>The angular spacing of features in the diffrac...</td>\n      <td>The angular spacing of features in the diffrac...</td>\n      <td>The angular spacing of features in the diffrac...</td>\n      <td>D</td>\n      <td>Kinematic diffraction is the approach to study...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# train validation test split\ntrain_df, temp_df = train_test_split(qna_df, test_size=0.5, random_state=seed)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:17:26.746693Z","iopub.execute_input":"2023-12-04T12:17:26.747004Z","iopub.status.idle":"2023-12-04T12:17:26.754065Z","shell.execute_reply.started":"2023-12-04T12:17:26.746978Z","shell.execute_reply":"2023-12-04T12:17:26.753220Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"# Data Loader\nCode is from Radek's notebook [here][1] with modifications to the tokenization process from Chris Deotte's notebook [here][2].\n\n[1]: https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training\n[2]: https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-1","metadata":{}},{"cell_type":"code","source":"# Mapping options to indices\noption_to_index = {option: idx for idx, option in enumerate('ABCDE')}\nindex_to_option = {v: k for k, v in option_to_index.items()}\n\ndef preprocess(example):\n    # Repeat the prompt for all five choices\n    first_sentence = [example['prompt']] * 5\n    # Extract sentences corresponding to choices 'A' to 'E'\n    second_sentences = [example[option] for option in 'ABCDE']\n    # Tokenize the sentences using the provided tokenizer (not shown in this snippet)\n    tokenized_example = tokenizer(first_sentence, second_sentences, truncation=False)\n    # Assign the index corresponding to the correct answer as the label\n    tokenized_example['label'] = option_to_index[example['answer']]\n\n    return tokenized_example\n\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n\n    def __call__(self, features):\n        # Determine the label name ('label' or 'labels')\n        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n        # Extract labels from each example and remove the corresponding key\n        labels = [feature.pop(label_name) for feature in features]\n        # Compute batch size and number of choices\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        # Restructure features into a list of dictionaries for each choice\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        # Flatten the list of dictionaries\n        flattened_features = sum(flattened_features, [])\n\n        # Tokenize and pad the examples into a batch\n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        # Reshape the batch to have dimensions (batch_size, num_choices, -1)\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        # Add labels to the batch as a PyTorch tensor\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        # Return the formatted batch\n        return batch","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:17:26.798935Z","iopub.execute_input":"2023-12-04T12:17:26.799643Z","iopub.status.idle":"2023-12-04T12:17:26.811593Z","shell.execute_reply.started":"2023-12-04T12:17:26.799617Z","shell.execute_reply":"2023-12-04T12:17:26.810618Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# Create tokenizer and datasets\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\ntrain_dataset = train_dataset.remove_columns([\"__index_level_0__\"])\n\ntrain_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:17:26.848168Z","iopub.execute_input":"2023-12-04T12:17:26.848699Z","iopub.status.idle":"2023-12-04T12:17:28.072383Z","shell.execute_reply.started":"2023-12-04T12:17:26.848675Z","shell.execute_reply":"2023-12-04T12:17:28.071300Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'context'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenize datasets\ntokenized_train_dataset = train_dataset.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_val_dataset = val_dataset.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n# We do not remove the answer column from the test dataset for evaluation\ntokenized_test_dataset = test_dataset.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E'])\n\ntokenized_train_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:17:28.074243Z","iopub.execute_input":"2023-12-04T12:17:28.074581Z","iopub.status.idle":"2023-12-04T12:17:28.416634Z","shell.execute_reply.started":"2023-12-04T12:17:28.074551Z","shell.execute_reply":"2023-12-04T12:17:28.415683Z"},"trusted":true},"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ede2fcf33674ab196e6177ea075fe44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4c265ddcdf449b0bb0f6b8e62571dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbd6a65937cc4a3a9d54c93e99927cfd"}},"metadata":{}},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Build Model\nWe will use a Hugging Face AutoModelForMultipleChoice. For the list of possible models, see Hugging Face's repository [here][1].  We can also optionally freeze layers. This also accelerates training and uses less memory. However validation accuracy may become less.\n\n[1]: https://huggingface.co/models","metadata":{}},{"cell_type":"code","source":"# Loading in the original deberta model\nmodel = AutoModelForMultipleChoice.from_pretrained(MODEL)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:17:28.417925Z","iopub.execute_input":"2023-12-04T12:17:28.418250Z","iopub.status.idle":"2023-12-04T12:17:32.451673Z","shell.execute_reply.started":"2023-12-04T12:17:28.418221Z","shell.execute_reply":"2023-12-04T12:17:32.450856Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"if FREEZE_EMBEDDINGS:\n    print('Freezing embeddings.')\n    for param in model.deberta.embeddings.parameters():\n        param.requires_grad = False\nif FREEZE_LAYERS>0:\n    print(f'Freezing {FREEZE_LAYERS} layers.')\n    for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n        for param in layer.parameters():\n            param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:17:32.454311Z","iopub.execute_input":"2023-12-04T12:17:32.454617Z","iopub.status.idle":"2023-12-04T12:17:32.462934Z","shell.execute_reply.started":"2023-12-04T12:17:32.454589Z","shell.execute_reply":"2023-12-04T12:17:32.461926Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Freezing embeddings.\nFreezing 18 layers.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# MAP@3 Metric\nThe competition metric is MAP@3 therefore we will make a custom code to add to Hugging Face's trainer. Discussion [here][1]\n\n[1]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/435602","metadata":{}},{"cell_type":"code","source":"def map_at_3(predictions, labels):\n    map_sum = 0\n    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n    for x,y in zip(pred,labels):\n        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n        map_sum += np.sum(z)\n    return map_sum / len(predictions)\n\n# Define metrics computation function for Hugging Face Trainer\ndef compute_metrics(p):\n    # computing the predictions and the labels\n    predictions, labels = p.predictions, p.label_ids\n\n    # Log multiple metrics: map@3 and accuracy\n    return {\"map@3\": map_at_3(predictions.tolist(), labels.tolist()),\n            \"accuracy\": accuracy_score(labels, predictions.argmax(axis=1))}","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:17:32.464169Z","iopub.execute_input":"2023-12-04T12:17:32.464508Z","iopub.status.idle":"2023-12-04T12:17:32.474368Z","shell.execute_reply.started":"2023-12-04T12:17:32.464476Z","shell.execute_reply":"2023-12-04T12:17:32.473414Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"# Train and Save \nWe will now train and save our model using Hugging Face's easy to use trainer.","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    warmup_ratio=0.1, \n    learning_rate=2e-5,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=2,\n    num_train_epochs=30,\n    report_to='none',\n    output_dir = f'./checkpoints_{VER}',\n    overwrite_output_dir=True,\n    fp16=True,\n    gradient_accumulation_steps=8,\n    logging_steps=25,\n    evaluation_strategy='steps',\n    eval_steps=25,\n    save_strategy=\"steps\",\n    save_steps=25,\n    load_best_model_at_end=True,\n    metric_for_best_model='map@3',\n    lr_scheduler_type='cosine',\n    weight_decay=0.01,\n    save_total_limit=2,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:17:32.475496Z","iopub.execute_input":"2023-12-04T12:17:32.475778Z","iopub.status.idle":"2023-12-04T12:17:32.489385Z","shell.execute_reply.started":"2023-12-04T12:17:32.475751Z","shell.execute_reply":"2023-12-04T12:17:32.488395Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_val_dataset,\n    compute_metrics = compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n)\n\n# Start training\ntrainer.train()\n# Save the trained model\ntrainer.save_model(f'model_v{VER}')","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:17:32.490674Z","iopub.execute_input":"2023-12-04T12:17:32.491043Z","iopub.status.idle":"2023-12-04T12:33:40.887466Z","shell.execute_reply.started":"2023-12-04T12:17:32.491018Z","shell.execute_reply":"2023-12-04T12:33:40.886238Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stderr","text":"You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [180/180 15:59, Epoch 28/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Map@3</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.617500</td>\n      <td>1.605330</td>\n      <td>0.566667</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.610000</td>\n      <td>1.602876</td>\n      <td>0.760000</td>\n      <td>0.640000</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.567900</td>\n      <td>1.364298</td>\n      <td>0.790000</td>\n      <td>0.680000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.882700</td>\n      <td>1.081310</td>\n      <td>0.723333</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.424600</td>\n      <td>1.147815</td>\n      <td>0.770000</td>\n      <td>0.660000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.293600</td>\n      <td>1.200993</td>\n      <td>0.770000</td>\n      <td>0.660000</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.262800</td>\n      <td>1.225123</td>\n      <td>0.776667</td>\n      <td>0.660000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Verify Saved Model\nDuring training, we see the MAP@3 validation score above. Let's load the saved model and compute it again here to verify that our model is saved correctly.","metadata":{}},{"cell_type":"code","source":"del model, trainer\nmodel = AutoModelForMultipleChoice.from_pretrained(f'model_v{VER}')\ntrainer = Trainer(model=model,\n                  tokenizer=tokenizer,\n                  data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer))","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:33:40.889482Z","iopub.execute_input":"2023-12-04T12:33:40.889838Z","iopub.status.idle":"2023-12-04T12:33:45.581602Z","shell.execute_reply.started":"2023-12-04T12:33:40.889808Z","shell.execute_reply":"2023-12-04T12:33:45.580524Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"test_predictions = trainer.predict(tokenized_test_dataset).predictions\npredictions_as_ids = np.argsort(-test_predictions, 1)\npredictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\npredictions_as_string = test_df['prediction'] = [\n    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n]","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:33:49.503526Z","iopub.execute_input":"2023-12-04T12:33:49.503885Z","iopub.status.idle":"2023-12-04T12:33:52.826535Z","shell.execute_reply.started":"2023-12-04T12:33:49.503855Z","shell.execute_reply":"2023-12-04T12:33:52.825725Z"},"trusted":true},"execution_count":67,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"markdown","source":"# Compute Validation Score","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking\nimport numpy as np\ndef precision_at_k(r, k):\n    \"\"\"Precision at k\"\"\"\n    assert k <= len(r)\n    assert k != 0\n    return sum(int(x) for x in r[:k]) / k\n\ndef MAP_at_3(predictions, true_items):\n    \"\"\"Score is mean average precision at 3\"\"\"\n    U = len(predictions)\n    map_at_3 = 0.0\n    for u in range(U):\n        user_preds = predictions[u].split()\n        user_true = true_items[u]\n        user_results = [1 if item == user_true else 0 for item in user_preds]\n        for k in range(min(len(user_preds), 3)):\n            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n    return map_at_3 / U","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:33:52.827827Z","iopub.execute_input":"2023-12-04T12:33:52.828218Z","iopub.status.idle":"2023-12-04T12:33:52.836433Z","shell.execute_reply.started":"2023-12-04T12:33:52.828168Z","shell.execute_reply":"2023-12-04T12:33:52.835455Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"m = MAP_at_3(test_df.prediction.values, test_df.answer.values)\nprint( 'Test MAP@3 =',m )","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:33:52.837818Z","iopub.execute_input":"2023-12-04T12:33:52.838209Z","iopub.status.idle":"2023-12-04T12:33:52.852452Z","shell.execute_reply.started":"2023-12-04T12:33:52.838181Z","shell.execute_reply":"2023-12-04T12:33:52.851588Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"CV MAP@3 = 0.77\n","output_type":"stream"}]},{"cell_type":"markdown","source":"MAP@3 is the official evaluaion metric of the competition, but out of curiosity let's check the accuracy too.","metadata":{}},{"cell_type":"code","source":"test_predictions = trainer.predict(tokenized_test_dataset).predictions\ntest_labels = [option_to_index[answer] for answer in tokenized_test_dataset['answer']]\n\ntest_accuracy = accuracy_score(test_labels, test_predictions.argmax(axis=1))\nprint( 'Test Accuracy =',test_accuracy )","metadata":{"execution":{"iopub.status.busy":"2023-12-04T12:45:36.828853Z","iopub.execute_input":"2023-12-04T12:45:36.829584Z","iopub.status.idle":"2023-12-04T12:45:40.117397Z","shell.execute_reply.started":"2023-12-04T12:45:36.829546Z","shell.execute_reply":"2023-12-04T12:45:40.116508Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Test Accuracy = 0.66\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}