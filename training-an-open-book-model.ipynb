{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":154040051,"sourceType":"kernelVersion"}],"dockerImageVersionId":30554,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/emmermarcell/training-an-open-book-model?scriptVersionId=154209282\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Training a model on the open book dataset\nThe second notebook of our homework contains the training process of the 'microsoft/deberta-v3-large' LLM on the open book database we created in the [context-creation][1] notebook. The entire notebook is heavily inspired from the popular competition notebook of the [Kaggle - LLM Science Exam][2], called [How To Train Open Book Model - Part 1][3]. There, [Chris Deotte][4] used a dataset containig 65k question and answer pairs with context compiled from different notebooks using e.g. gpt-3.5 turbo for data augmentation. This augmentation step was left out in our solution due to time constraints but it's an obvious choice of improvement in the future.\n\nWe use the 'microsoft/deberta-v3-large' on the openbook data simply because it turned out to be one of the most successful model in terms of highest [notebook scores][5] on kaggle\n\n\n[1]: https://www.kaggle.com/code/emmermarcell/context-creation\n[2]: https://www.kaggle.com/competitions/kaggle-llm-science-exam\n[3]: https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-1\n[4]: https://www.kaggle.com/cdeotte\n[5]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/models","metadata":{}},{"cell_type":"code","source":"!pip install -qq wandb --upgrade","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:43:08.260147Z","iopub.execute_input":"2023-12-08T22:43:08.260443Z","iopub.status.idle":"2023-12-08T22:43:24.82253Z","shell.execute_reply.started":"2023-12-08T22:43:08.260418Z","shell.execute_reply":"2023-12-08T22:43:24.821271Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Load CSV\n\nFirst we load in  the csv file generated in the [context-creation][1] notebook and split it into a training, validation, and a test set.\n\n[1]: https://www.kaggle.com/code/emmermarcell/context-creation","metadata":{}},{"cell_type":"code","source":"import os\n\n# Set CUDA visible devices to GPU 0 and 1 (The 2xT4 GPUs that Kaggle provides)\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n\nfrom typing import Optional, Union\nimport pandas as pd, numpy as np, torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom datasets import Dataset, load_metric\nfrom dataclasses import dataclass\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForMultipleChoice, EarlyStoppingCallback, \\\n                         TrainingArguments, Trainer, set_seed\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n\n# Random seed\nseed = 42\nset_seed(seed)\n# Define constants\nVER=1\n# Number of layers to freeze, DeBERTa has a total number of 24 layers\nFREEZE_LAYERS = 18\n# Boolean to freeze embeddings\nFREEZE_EMBEDDINGS = True\n# Length of context + question + answers\nMAX_INPUT = 256\n# The Hugging Face model we're using\nMODEL = 'microsoft/deberta-v3-large'","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:43:24.824723Z","iopub.execute_input":"2023-12-08T22:43:24.825132Z","iopub.status.idle":"2023-12-08T22:43:41.263593Z","shell.execute_reply.started":"2023-12-08T22:43:24.825096Z","shell.execute_reply":"2023-12-08T22:43:41.26261Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import Weights & Biases library\nimport wandb\n\nwandb.login()\n\n\n%env WANDB_PROJECT=llm_science_exam_open_book_approach\n%env WANDB_LOG_MODEL=true","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:43:41.26481Z","iopub.execute_input":"2023-12-08T22:43:41.265151Z","iopub.status.idle":"2023-12-08T22:44:31.268774Z","shell.execute_reply.started":"2023-12-08T22:43:41.265119Z","shell.execute_reply":"2023-12-08T22:44:31.267597Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"env: WANDB_PROJECT=llm_science_exam_open_book_approach\nenv: WANDB_LOG_MODEL=true\n","output_type":"stream"}]},{"cell_type":"code","source":"# Read validation data from a CSV file\nqna_df = pd.read_csv('/kaggle/input/context-creation/openbook-qna-data.csv')\nprint('Validation data size:', qna_df.shape )\nqna_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:44:31.271038Z","iopub.execute_input":"2023-12-08T22:44:31.271686Z","iopub.status.idle":"2023-12-08T22:44:31.318452Z","shell.execute_reply.started":"2023-12-08T22:44:31.271655Z","shell.execute_reply":"2023-12-08T22:44:31.317575Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Validation data size: (200, 8)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  Which of the following statements accurately d...   \n1  Which of the following is an accurate definiti...   \n2  Which of the following statements accurately d...   \n3  What is the significance of regularization in ...   \n4  Which of the following statements accurately d...   \n\n                                                   A  \\\n0  MOND is a theory that reduces the observed mis...   \n1  Dynamic scaling refers to the evolution of sel...   \n2  The triskeles symbol was reconstructed as a fe...   \n3  Regularizing the mass-energy of an electron wi...   \n4  The angular spacing of features in the diffrac...   \n\n                                                   B  \\\n0  MOND is a theory that increases the discrepanc...   \n1  Dynamic scaling refers to the non-evolution of...   \n2  The triskeles symbol is a representation of th...   \n3  Regularizing the mass-energy of an electron wi...   \n4  The angular spacing of features in the diffrac...   \n\n                                                   C  \\\n0  MOND is a theory that explains the missing bar...   \n1  Dynamic scaling refers to the evolution of sel...   \n2  The triskeles symbol is a representation of a ...   \n3  Regularizing the mass-energy of an electron wi...   \n4  The angular spacing of features in the diffrac...   \n\n                                                   D  \\\n0  MOND is a theory that reduces the discrepancy ...   \n1  Dynamic scaling refers to the non-evolution of...   \n2  The triskeles symbol represents three interloc...   \n3  Regularizing the mass-energy of an electron wi...   \n4  The angular spacing of features in the diffrac...   \n\n                                                   E answer  \\\n0  MOND is a theory that eliminates the observed ...      D   \n1  Dynamic scaling refers to the evolution of sel...      A   \n2  The triskeles symbol is a representation of th...      A   \n3  Regularizing the mass-energy of an electron wi...      C   \n4  The angular spacing of features in the diffrac...      D   \n\n                                             context  \n0  In cosmology, the missing baryon problem is an...  \n1  Dynamic scaling (sometimes known as Family-Vic...  \n2  thumb|Neolithic triple spiral symbol A triskel...  \n3  In physics, especially quantum field theory, r...  \n4  Kinematic diffraction is the approach to study...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>D</th>\n      <th>E</th>\n      <th>answer</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Which of the following statements accurately d...</td>\n      <td>MOND is a theory that reduces the observed mis...</td>\n      <td>MOND is a theory that increases the discrepanc...</td>\n      <td>MOND is a theory that explains the missing bar...</td>\n      <td>MOND is a theory that reduces the discrepancy ...</td>\n      <td>MOND is a theory that eliminates the observed ...</td>\n      <td>D</td>\n      <td>In cosmology, the missing baryon problem is an...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Which of the following is an accurate definiti...</td>\n      <td>Dynamic scaling refers to the evolution of sel...</td>\n      <td>Dynamic scaling refers to the non-evolution of...</td>\n      <td>Dynamic scaling refers to the evolution of sel...</td>\n      <td>Dynamic scaling refers to the non-evolution of...</td>\n      <td>Dynamic scaling refers to the evolution of sel...</td>\n      <td>A</td>\n      <td>Dynamic scaling (sometimes known as Family-Vic...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Which of the following statements accurately d...</td>\n      <td>The triskeles symbol was reconstructed as a fe...</td>\n      <td>The triskeles symbol is a representation of th...</td>\n      <td>The triskeles symbol is a representation of a ...</td>\n      <td>The triskeles symbol represents three interloc...</td>\n      <td>The triskeles symbol is a representation of th...</td>\n      <td>A</td>\n      <td>thumb|Neolithic triple spiral symbol A triskel...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>What is the significance of regularization in ...</td>\n      <td>Regularizing the mass-energy of an electron wi...</td>\n      <td>Regularizing the mass-energy of an electron wi...</td>\n      <td>Regularizing the mass-energy of an electron wi...</td>\n      <td>Regularizing the mass-energy of an electron wi...</td>\n      <td>Regularizing the mass-energy of an electron wi...</td>\n      <td>C</td>\n      <td>In physics, especially quantum field theory, r...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Which of the following statements accurately d...</td>\n      <td>The angular spacing of features in the diffrac...</td>\n      <td>The angular spacing of features in the diffrac...</td>\n      <td>The angular spacing of features in the diffrac...</td>\n      <td>The angular spacing of features in the diffrac...</td>\n      <td>The angular spacing of features in the diffrac...</td>\n      <td>D</td>\n      <td>Kinematic diffraction is the approach to study...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# train validation test split\ntrain_df, temp_df = train_test_split(qna_df, test_size=0.5, random_state=seed)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:44:31.319491Z","iopub.execute_input":"2023-12-08T22:44:31.319755Z","iopub.status.idle":"2023-12-08T22:44:31.330381Z","shell.execute_reply.started":"2023-12-08T22:44:31.31973Z","shell.execute_reply":"2023-12-08T22:44:31.3294Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Data Loader\nWe use a custom class for tokenizing the input data with dynamic padding in order to use it as an input to the ['microsoft/deberta-v3-large'][1] model.\n\nThe implementation of the preprocess function and the DataCollatorForMultipleChoice class is not ours, it's from Radek's notebook [here][2] with modifications to the tokenization process from Chris Deotte's notebook [here][3].\n\n[1]: https://huggingface.co/microsoft/deberta-v3-large\n[2]: https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training\n[3]: https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-1","metadata":{}},{"cell_type":"code","source":"# Mapping options to indices\noption_to_index = {option: idx for idx, option in enumerate('ABCDE')}\nindex_to_option = {v: k for k, v in option_to_index.items()}\n\ndef preprocess(example):\n    # Repeat the prompt for all five choices\n    first_sentence = [example['prompt']] * 5\n    # Extract sentences corresponding to choices 'A' to 'E'\n    second_sentences = [example[option] for option in 'ABCDE']\n    # Tokenize the sentences using the provided tokenizer (not shown in this snippet)\n    tokenized_example = tokenizer(first_sentence, second_sentences, truncation=False)\n    # Assign the index corresponding to the correct answer as the label\n    tokenized_example['label'] = option_to_index[example['answer']]\n\n    return tokenized_example\n\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n\n    def __call__(self, features):\n        # Determine the label name ('label' or 'labels')\n        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n        # Extract labels from each example and remove the corresponding key\n        labels = [feature.pop(label_name) for feature in features]\n        # Compute batch size and number of choices\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        # Restructure features into a list of dictionaries for each choice\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        # Flatten the list of dictionaries\n        flattened_features = sum(flattened_features, [])\n\n        # Tokenize and pad the examples into a batch\n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        # Reshape the batch to have dimensions (batch_size, num_choices, -1)\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        # Add labels to the batch as a PyTorch tensor\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        # Return the formatted batch\n        return batch","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:44:31.331834Z","iopub.execute_input":"2023-12-08T22:44:31.332183Z","iopub.status.idle":"2023-12-08T22:44:31.344766Z","shell.execute_reply.started":"2023-12-08T22:44:31.33215Z","shell.execute_reply":"2023-12-08T22:44:31.343937Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Create tokenizer and datasets\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\ntrain_dataset = train_dataset.remove_columns([\"__index_level_0__\"])\n\ntrain_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:44:31.346002Z","iopub.execute_input":"2023-12-08T22:44:31.34646Z","iopub.status.idle":"2023-12-08T22:44:33.721445Z","shell.execute_reply.started":"2023-12-08T22:44:31.346404Z","shell.execute_reply":"2023-12-08T22:44:33.720568Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d572176ee9214bdd81a2c0bda255f379"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c40a8ce09ae44461aaa7da49eec39b67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"567795a5cd26417d9bcfce0668b57136"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'A', 'B', 'C', 'D', 'E', 'answer', 'context'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenize datasets\ntokenized_train_dataset = train_dataset.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_val_dataset = val_dataset.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n# We do not remove the answer column from the test dataset for evaluation\ntokenized_test_dataset = test_dataset.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E'])\n\ntokenized_train_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:44:33.722754Z","iopub.execute_input":"2023-12-08T22:44:33.723108Z","iopub.status.idle":"2023-12-08T22:44:34.079897Z","shell.execute_reply.started":"2023-12-08T22:44:33.723075Z","shell.execute_reply":"2023-12-08T22:44:34.079072Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b81742f399a4037a485c313fab67d52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68922a8cf2914eae9467fae70210b073"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8685f1a3cc06457c90446d5e71fa6806"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n    num_rows: 100\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Building the model","metadata":{}},{"cell_type":"code","source":"def model_init():\n    # Loading in the deberta model\n    model = AutoModelForMultipleChoice.from_pretrained(MODEL)\n    # We freeze the first 18 layers of the model for faster training time. \n    # However, his is compensated by lower valuation accuracy.\n    if FREEZE_EMBEDDINGS:\n        print('Freezing embeddings.')\n        for param in model.deberta.embeddings.parameters():\n            param.requires_grad = False\n    if FREEZE_LAYERS>0:\n        print(f'Freezing {FREEZE_LAYERS} layers.')\n        for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n            for param in layer.parameters():\n                param.requires_grad = False\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:44:34.081277Z","iopub.execute_input":"2023-12-08T22:44:34.081664Z","iopub.status.idle":"2023-12-08T22:44:34.087804Z","shell.execute_reply.started":"2023-12-08T22:44:34.081629Z","shell.execute_reply":"2023-12-08T22:44:34.086939Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Exploring Hyperparameter Combinations With Sweeps","metadata":{}},{"cell_type":"code","source":"# method and metric\nsweep_config = {\n    'method': 'bayes',\n    'metric': {\n        'name': 'map@3',\n        'goal': 'maximize'\n    },\n}\n\n\n# hyperparameters\nparameters_dict = {\n    'learning_rate': {\n      'min': 1e-6,\n      'max': 1e-4\n    },\n   'epochs':{\n      'values': [20, 30, 50]\n   },\n    'weight_decay': {\n      'values': [0.0, 0.01, 0.02, 0.03, 0.04, 0.05]\n    },\n    'warmup_ratio': {\n      'values': [0.0, 0.05, 0.1, 0.15, 0.2]\n    },\n    'gradient_accumulation_steps': {\n      'values': [2, 4, 8, 16]\n    },\n    'early_stopping_patience': {\n      'values': [5, 10]\n    },\n}\n\n\nsweep_config['parameters'] = parameters_dict","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:44:34.09137Z","iopub.execute_input":"2023-12-08T22:44:34.091619Z","iopub.status.idle":"2023-12-08T22:44:34.098527Z","shell.execute_reply.started":"2023-12-08T22:44:34.091598Z","shell.execute_reply":"2023-12-08T22:44:34.097761Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project='llm_science_exam_open_book_approach')","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:44:34.099529Z","iopub.execute_input":"2023-12-08T22:44:34.099801Z","iopub.status.idle":"2023-12-08T22:44:34.434693Z","shell.execute_reply.started":"2023-12-08T22:44:34.099778Z","shell.execute_reply":"2023-12-08T22:44:34.433748Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Create sweep with ID: o017v6q1\nSweep URL: https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# MAP@3 Metric\nThe competition metric is MAP@3 therefore we will make a custom code to add to Hugging Face's trainer. Discussion [here][1]\n\n[1]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/435602","metadata":{}},{"cell_type":"code","source":"def map_at_3(predictions, labels):\n    map_sum = 0\n    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n    for x,y in zip(pred,labels):\n        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n        map_sum += np.sum(z)\n    return map_sum / len(predictions)\n\n# Define metrics computation function for Hugging Face Trainer\ndef compute_metrics(p):\n    # computing the predictions and the labels\n    predictions, labels = p.predictions, p.label_ids\n\n    # Log multiple metrics: map@3 and accuracy\n    return {\"map@3\": map_at_3(predictions.tolist(), labels.tolist()),\n            \"accuracy\": accuracy_score(labels, predictions.argmax(axis=1))}","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:44:34.4357Z","iopub.execute_input":"2023-12-08T22:44:34.435972Z","iopub.status.idle":"2023-12-08T22:44:34.442898Z","shell.execute_reply.started":"2023-12-08T22:44:34.435946Z","shell.execute_reply":"2023-12-08T22:44:34.442068Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Compute Validation Score","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking\nimport numpy as np\ndef precision_at_k(r, k):\n    \"\"\"Precision at k\"\"\"\n    assert k <= len(r)\n    assert k != 0\n    return sum(int(x) for x in r[:k]) / k\n\ndef MAP_at_3(predictions, true_items):\n    \"\"\"Score is mean average precision at 3\"\"\"\n    U = len(predictions)\n    map_at_3 = 0.0\n    for u in range(U):\n        user_preds = predictions[u].split()\n        user_true = true_items[u]\n        user_results = [1 if item == user_true else 0 for item in user_preds]\n        for k in range(min(len(user_preds), 3)):\n            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n    return map_at_3 / U","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:44:34.443923Z","iopub.execute_input":"2023-12-08T22:44:34.44418Z","iopub.status.idle":"2023-12-08T22:44:34.454114Z","shell.execute_reply.started":"2023-12-08T22:44:34.444157Z","shell.execute_reply":"2023-12-08T22:44:34.453363Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Train and Save \nWe now train our model using the Hugging Face Trainer API and leverage Weights & Biases (W&B) Sweeps to perform hyperparameter search. A great article that describes this method can be found [here][1].\n\n[1]: https://wandb.ai/matt24/vit-snacks-sweeps/reports/Hyperparameter-Search-for-HuggingFace-Transformer-Models--VmlldzoyMTUxNTg0","metadata":{}},{"cell_type":"code","source":"def train(config=None):\n    with wandb.init(config=config):\n        # set sweep configuration\n        config = wandb.config\n        \n        # Run the wandb magic comand!\n        # This displays the details of each training\n        %wandb\n\n\n        # set training arguments\n        training_args = TrainingArguments(\n            output_dir = f'/kaggle/working/checkpoints_{VER}',\n            report_to='wandb',\n            num_train_epochs=config.epochs,\n            learning_rate=config.learning_rate,\n            weight_decay=config.weight_decay,\n            warmup_ratio=config.warmup_ratio,\n            gradient_accumulation_steps=config.gradient_accumulation_steps,\n            per_device_train_batch_size=1,\n            per_device_eval_batch_size=2,\n            overwrite_output_dir=True,\n            fp16=True,\n            logging_steps=25,\n            evaluation_strategy='steps',\n            eval_steps=25,\n            save_strategy=\"steps\",\n            save_steps=25,\n            load_best_model_at_end=True,\n            metric_for_best_model='map@3',\n            lr_scheduler_type='cosine',\n            save_total_limit=2,\n        )\n\n\n        # define training loop\n        trainer = Trainer(\n            model_init=model_init,\n            args=training_args,\n            tokenizer=tokenizer,\n            data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n            train_dataset=tokenized_train_dataset,\n            eval_dataset=tokenized_val_dataset,\n            compute_metrics = compute_metrics,\n            callbacks=[EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)],\n        )\n        \n\n        # start training loop\n        trainer.train()\n        \n        \n        # Save the trained model\n        trainer.save_model(f'/kaggle/working/model_v{VER}')\n        \n        \n        # Free up space\n        del model, trainer\n        gc.collect()\n        \n        model = AutoModelForMultipleChoice.from_pretrained(f'/kaggle/working/model_v{VER}')\n        trainer = Trainer(model=model,\n                          tokenizer=tokenizer,\n                          data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer))\n        \n        # Verify Saved Model\n        test_predictions = trainer.predict(tokenized_test_dataset).predictions\n        predictions_as_ids = np.argsort(-test_predictions, 1)\n        predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n        predictions_as_string = test_df['prediction'] = [\n            ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n        ]\n        test_labels = [option_to_index[answer] for answer in tokenized_test_dataset['answer']]\n        \n        m = MAP_at_3(test_df.prediction.values, test_df.answer.values)\n        test_accuracy = accuracy_score(test_labels, test_predictions.argmax(axis=1))\n        \n        # Log the metrics on the test set\n        wandb.log({'Test MAP@3': m,\n                  'Test Accuracy': test_accuracy})\n        \n        print('Test MAP@3 = ',m)\n        print('Test Accuracy = ', test_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:44:34.455375Z","iopub.execute_input":"2023-12-08T22:44:34.45586Z","iopub.status.idle":"2023-12-08T22:44:34.474019Z","shell.execute_reply.started":"2023-12-08T22:44:34.455828Z","shell.execute_reply":"2023-12-08T22:44:34.473206Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Run the sweep agent\nwandb.agent(sweep_id, train, count=5)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T22:44:34.474906Z","iopub.execute_input":"2023-12-08T22:44:34.475172Z","iopub.status.idle":"2023-12-09T00:07:11.935211Z","shell.execute_reply.started":"2023-12-08T22:44:34.475149Z","shell.execute_reply":"2023-12-09T00:07:11.934306Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3xvws7p7 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.2034401103388e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.04\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memmermarci\u001b[0m (\u001b[33mimport_this\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\ncat: /sys/module/amdgpu/initstate: No such file or directory\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231208_224436-3xvws7p7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/3xvws7p7' target=\"_blank\">scarlet-sweep-1</a></strong> to <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/3xvws7p7' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/3xvws7p7</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<wandb.jupyter.IFrame at 0x7ae3a744e500>","text/html":"<iframe src='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/3xvws7p7?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f3da468355d4e83aefabb1bee0509c7"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Freezing embeddings.\nFreezing 18 layers.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Freezing embeddings.\nFreezing 18 layers.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\nYou're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='275' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [275/300 26:27 < 02:25, 0.17 it/s, Epoch 44/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Map@3</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.609400</td>\n      <td>1.568839</td>\n      <td>0.760000</td>\n      <td>0.620000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.805300</td>\n      <td>1.538824</td>\n      <td>0.740000</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.161600</td>\n      <td>1.747755</td>\n      <td>0.706667</td>\n      <td>0.540000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.040100</td>\n      <td>2.641541</td>\n      <td>0.720000</td>\n      <td>0.540000</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.041500</td>\n      <td>3.035136</td>\n      <td>0.700000</td>\n      <td>0.520000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.004100</td>\n      <td>3.714784</td>\n      <td>0.690000</td>\n      <td>0.520000</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.009900</td>\n      <td>2.432216</td>\n      <td>0.716667</td>\n      <td>0.540000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.000200</td>\n      <td>3.054488</td>\n      <td>0.723333</td>\n      <td>0.520000</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.000000</td>\n      <td>2.925502</td>\n      <td>0.686667</td>\n      <td>0.500000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.000200</td>\n      <td>2.917889</td>\n      <td>0.726667</td>\n      <td>0.560000</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.000000</td>\n      <td>2.997889</td>\n      <td>0.736667</td>\n      <td>0.560000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_32/2400146832.py\", line 58, in train\n    del model, trainer\nUnboundLocalError: local variable 'model' referenced before assignment\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='1670.376 MB of 1670.376 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>█▆▃▃▂▂▃▂▁▅▅</td></tr><tr><td>eval/loss</td><td>▁▁▂▅▆█▄▆▅▅▆</td></tr><tr><td>eval/map@3</td><td>█▆▃▄▂▁▄▅▁▅▆</td></tr><tr><td>eval/runtime</td><td>▅▁▇▁▁▁▁▆▁▁█</td></tr><tr><td>eval/samples_per_second</td><td>▄█▂████▃██▁</td></tr><tr><td>eval/steps_per_second</td><td>▄█▂████▃██▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▅▄▃▂▁▁</td></tr><tr><td>train/loss</td><td>█▅▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.56</td></tr><tr><td>eval/loss</td><td>2.99789</td></tr><tr><td>eval/map@3</td><td>0.73667</td></tr><tr><td>eval/runtime</td><td>5.1936</td></tr><tr><td>eval/samples_per_second</td><td>9.627</td></tr><tr><td>eval/steps_per_second</td><td>2.503</td></tr><tr><td>train/epoch</td><td>44.0</td></tr><tr><td>train/global_step</td><td>275</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0</td></tr><tr><td>train/total_flos</td><td>2686355819036340.0</td></tr><tr><td>train/train_loss</td><td>0.24295</td></tr><tr><td>train/train_runtime</td><td>1557.0818</td></tr><tr><td>train/train_samples_per_second</td><td>3.211</td></tr><tr><td>train/train_steps_per_second</td><td>0.193</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">scarlet-sweep-1</strong> at: <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/3xvws7p7' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/3xvws7p7</a><br/>Synced 6 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231208_224436-3xvws7p7/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run 3xvws7p7 errored: UnboundLocalError(\"local variable 'model' referenced before assignment\")\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 3xvws7p7 errored: UnboundLocalError(\"local variable 'model' referenced before assignment\")\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vr44p2h3 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 8\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.623632873706874e-06\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\ncat: /sys/module/amdgpu/initstate: No such file or directory\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231208_231237-vr44p2h3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/vr44p2h3' target=\"_blank\">dauntless-sweep-2</a></strong> to <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/vr44p2h3' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/vr44p2h3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<wandb.jupyter.IFrame at 0x7ae3a86b4730>","text/html":"<iframe src='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/vr44p2h3?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Freezing embeddings.\nFreezing 18 layers.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Freezing embeddings.\nFreezing 18 layers.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 28:59, Epoch 48/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Map@3</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.617700</td>\n      <td>1.610545</td>\n      <td>0.283333</td>\n      <td>0.140000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.617700</td>\n      <td>1.609479</td>\n      <td>0.430000</td>\n      <td>0.260000</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.611500</td>\n      <td>1.608089</td>\n      <td>0.583333</td>\n      <td>0.400000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.612000</td>\n      <td>1.606702</td>\n      <td>0.636667</td>\n      <td>0.480000</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.606500</td>\n      <td>1.604766</td>\n      <td>0.693333</td>\n      <td>0.540000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.605600</td>\n      <td>1.600988</td>\n      <td>0.693333</td>\n      <td>0.540000</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.597200</td>\n      <td>1.593765</td>\n      <td>0.680000</td>\n      <td>0.540000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.590800</td>\n      <td>1.587190</td>\n      <td>0.716667</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.589100</td>\n      <td>1.577128</td>\n      <td>0.716667</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.561900</td>\n      <td>1.566447</td>\n      <td>0.706667</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.567300</td>\n      <td>1.563383</td>\n      <td>0.706667</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.560800</td>\n      <td>1.562894</td>\n      <td>0.706667</td>\n      <td>0.580000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_32/2400146832.py\", line 58, in train\n    del model, trainer\nUnboundLocalError: local variable 'model' referenced before assignment\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='1670.376 MB of 1670.376 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▃▅▆▇▇▇█████</td></tr><tr><td>eval/loss</td><td>███▇▇▇▆▅▃▂▁▁</td></tr><tr><td>eval/map@3</td><td>▁▃▆▇██▇█████</td></tr><tr><td>eval/runtime</td><td>▁▂▁▂▂█▂▂▂▁▂▂</td></tr><tr><td>eval/samples_per_second</td><td>█▇█▇▇▁▇▇▇█▇▇</td></tr><tr><td>eval/steps_per_second</td><td>█▇█▇▇▁▇▇▇█▇▇</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▅▄▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▇▅▅▄▁▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.58</td></tr><tr><td>eval/loss</td><td>1.56289</td></tr><tr><td>eval/map@3</td><td>0.70667</td></tr><tr><td>eval/runtime</td><td>5.0374</td></tr><tr><td>eval/samples_per_second</td><td>9.926</td></tr><tr><td>eval/steps_per_second</td><td>2.581</td></tr><tr><td>train/epoch</td><td>48.0</td></tr><tr><td>train/global_step</td><td>300</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.5608</td></tr><tr><td>train/total_flos</td><td>2931643893306660.0</td></tr><tr><td>train/train_loss</td><td>1.59484</td></tr><tr><td>train/train_runtime</td><td>1704.3577</td></tr><tr><td>train/train_samples_per_second</td><td>2.934</td></tr><tr><td>train/train_steps_per_second</td><td>0.176</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dauntless-sweep-2</strong> at: <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/vr44p2h3' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/vr44p2h3</a><br/>Synced 6 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231208_231237-vr44p2h3/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run vr44p2h3 errored: UnboundLocalError(\"local variable 'model' referenced before assignment\")\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run vr44p2h3 errored: UnboundLocalError(\"local variable 'model' referenced before assignment\")\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m9chx59e with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.728546499681623e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.04\ncat: /sys/module/amdgpu/initstate: No such file or directory\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231208_234300-m9chx59e</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/m9chx59e' target=\"_blank\">genial-sweep-3</a></strong> to <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/m9chx59e' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/m9chx59e</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<wandb.jupyter.IFrame at 0x7ae36127ad40>","text/html":"<iframe src='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/m9chx59e?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Freezing embeddings.\nFreezing 18 layers.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Freezing embeddings.\nFreezing 18 layers.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='325' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [325/500 09:55 < 05:22, 0.54 it/s, Epoch 13/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Map@3</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.615000</td>\n      <td>1.614609</td>\n      <td>0.333333</td>\n      <td>0.240000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.615700</td>\n      <td>1.591963</td>\n      <td>0.720000</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.604400</td>\n      <td>1.602128</td>\n      <td>0.813333</td>\n      <td>0.740000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.420400</td>\n      <td>1.505946</td>\n      <td>0.653333</td>\n      <td>0.520000</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.060400</td>\n      <td>1.317188</td>\n      <td>0.696667</td>\n      <td>0.540000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.635700</td>\n      <td>1.202507</td>\n      <td>0.693333</td>\n      <td>0.560000</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.405100</td>\n      <td>1.262098</td>\n      <td>0.743333</td>\n      <td>0.660000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.397400</td>\n      <td>2.076625</td>\n      <td>0.726667</td>\n      <td>0.560000</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.270500</td>\n      <td>1.889822</td>\n      <td>0.753333</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.151300</td>\n      <td>1.964887</td>\n      <td>0.726667</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.122000</td>\n      <td>1.647047</td>\n      <td>0.743333</td>\n      <td>0.600000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.147800</td>\n      <td>2.198858</td>\n      <td>0.730000</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.115100</td>\n      <td>2.305092</td>\n      <td>0.776667</td>\n      <td>0.640000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_32/2400146832.py\", line 58, in train\n    del model, trainer\nUnboundLocalError: local variable 'model' referenced before assignment\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='1670.376 MB of 1670.376 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆█▅▅▅▇▅▆▆▆▆▇</td></tr><tr><td>eval/loss</td><td>▄▃▄▃▂▁▁▇▅▆▄▇█</td></tr><tr><td>eval/map@3</td><td>▁▇█▆▆▆▇▇▇▇▇▇▇</td></tr><tr><td>eval/runtime</td><td>█▁▄▂▃▃▃▃▂▄▃▄▃</td></tr><tr><td>eval/samples_per_second</td><td>▁█▅▇▆▆▆▆▇▅▆▅▆</td></tr><tr><td>eval/steps_per_second</td><td>▁█▅▆▆▆▆▆▆▅▆▅▆</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███▇▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>███▇▅▃▂▂▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.64</td></tr><tr><td>eval/loss</td><td>2.30509</td></tr><tr><td>eval/map@3</td><td>0.77667</td></tr><tr><td>eval/runtime</td><td>5.0502</td></tr><tr><td>eval/samples_per_second</td><td>9.901</td></tr><tr><td>eval/steps_per_second</td><td>2.574</td></tr><tr><td>train/epoch</td><td>13.0</td></tr><tr><td>train/global_step</td><td>325</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.1151</td></tr><tr><td>train/total_flos</td><td>794310349738980.0</td></tr><tr><td>train/train_loss</td><td>0.73545</td></tr><tr><td>train/train_runtime</td><td>557.7131</td></tr><tr><td>train/train_samples_per_second</td><td>3.586</td></tr><tr><td>train/train_steps_per_second</td><td>0.897</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">genial-sweep-3</strong> at: <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/m9chx59e' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/m9chx59e</a><br/>Synced 6 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231208_234300-m9chx59e/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run m9chx59e errored: UnboundLocalError(\"local variable 'model' referenced before assignment\")\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run m9chx59e errored: UnboundLocalError(\"local variable 'model' referenced before assignment\")\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o395ip9l with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.34491372502644e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.04\ncat: /sys/module/amdgpu/initstate: No such file or directory\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231208_235411-o395ip9l</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/o395ip9l' target=\"_blank\">dazzling-sweep-4</a></strong> to <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/o395ip9l' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/o395ip9l</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<wandb.jupyter.IFrame at 0x7ae361142170>","text/html":"<iframe src='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/o395ip9l?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Freezing embeddings.\nFreezing 18 layers.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Freezing embeddings.\nFreezing 18 layers.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='350' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [350/500 10:40 < 04:35, 0.54 it/s, Epoch 14/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Map@3</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.614900</td>\n      <td>1.620336</td>\n      <td>0.386667</td>\n      <td>0.240000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.594400</td>\n      <td>1.526233</td>\n      <td>0.750000</td>\n      <td>0.600000</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.383900</td>\n      <td>1.468326</td>\n      <td>0.506667</td>\n      <td>0.380000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.008600</td>\n      <td>1.218655</td>\n      <td>0.843333</td>\n      <td>0.760000</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.727300</td>\n      <td>1.075755</td>\n      <td>0.753333</td>\n      <td>0.620000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.635000</td>\n      <td>1.108320</td>\n      <td>0.740000</td>\n      <td>0.580000</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.294600</td>\n      <td>1.055765</td>\n      <td>0.810000</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.398200</td>\n      <td>1.793868</td>\n      <td>0.770000</td>\n      <td>0.640000</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.116500</td>\n      <td>1.956651</td>\n      <td>0.773333</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.136300</td>\n      <td>1.481208</td>\n      <td>0.813333</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.058000</td>\n      <td>2.040640</td>\n      <td>0.816667</td>\n      <td>0.700000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.028900</td>\n      <td>2.581166</td>\n      <td>0.790000</td>\n      <td>0.660000</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.031900</td>\n      <td>2.934046</td>\n      <td>0.766667</td>\n      <td>0.620000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.022300</td>\n      <td>2.861752</td>\n      <td>0.753333</td>\n      <td>0.600000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_32/2400146832.py\", line 58, in train\n    del model, trainer\nUnboundLocalError: local variable 'model' referenced before assignment\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='1670.376 MB of 1670.376 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆▃█▆▆▇▆▇▇▇▇▆▆</td></tr><tr><td>eval/loss</td><td>▃▃▃▂▁▁▁▄▄▃▅▇██</td></tr><tr><td>eval/map@3</td><td>▁▇▃█▇▆▇▇▇██▇▇▇</td></tr><tr><td>eval/runtime</td><td>▂▂▂▃▁▂▂▁▂█▂▁▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▇▇▇▆█▇▇█▇▁▇██▇</td></tr><tr><td>eval/steps_per_second</td><td>▇▇▇▆█▇▇█▇▁▇██▇</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▃███▇▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>██▇▅▄▄▂▃▁▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.6</td></tr><tr><td>eval/loss</td><td>2.86175</td></tr><tr><td>eval/map@3</td><td>0.75333</td></tr><tr><td>eval/runtime</td><td>5.0438</td></tr><tr><td>eval/samples_per_second</td><td>9.913</td></tr><tr><td>eval/steps_per_second</td><td>2.577</td></tr><tr><td>train/epoch</td><td>14.0</td></tr><tr><td>train/global_step</td><td>350</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.0223</td></tr><tr><td>train/total_flos</td><td>853466348653980.0</td></tr><tr><td>train/train_loss</td><td>0.57505</td></tr><tr><td>train/train_runtime</td><td>603.8214</td></tr><tr><td>train/train_samples_per_second</td><td>3.312</td></tr><tr><td>train/train_steps_per_second</td><td>0.828</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">dazzling-sweep-4</strong> at: <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/o395ip9l' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/o395ip9l</a><br/>Synced 6 W&B file(s), 0 media file(s), 8 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231208_235411-o395ip9l/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run o395ip9l errored: UnboundLocalError(\"local variable 'model' referenced before assignment\")\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run o395ip9l errored: UnboundLocalError(\"local variable 'model' referenced before assignment\")\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ptd1z34q with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_patience: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n\u001b[34m\u001b[1mwandb\u001b[0m: \tgradient_accumulation_steps: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.3952780399567949e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_ratio: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.04\ncat: /sys/module/amdgpu/initstate: No such file or directory\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231209_000608-ptd1z34q</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/ptd1z34q' target=\"_blank\">radiant-sweep-5</a></strong> to <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/sweeps/o017v6q1</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/ptd1z34q' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/ptd1z34q</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<wandb.jupyter.IFrame at 0x7ae39c314880>","text/html":"<iframe src='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/ptd1z34q?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Freezing embeddings.\nFreezing 18 layers.\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Freezing embeddings.\nFreezing 18 layers.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'warmup_ratio' was locked by 'sweep' (ignored update).\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_32/2400146832.py\", line 50, in train\n    trainer.train()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1555, in train\n    return inner_training_loop(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1837, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2693, in training_step\n    self.accelerator.backward(loss)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 1921, in backward\n    self.scaler.scale(loss).backward(**kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n    torch.autograd.backward(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 200, in backward\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py\", line 274, in apply\n    return user_fn(self, *args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py\", line 34, in backward\n    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py\", line 506, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py\", line 45, in forward\n    return comm.reduce_add_coalesced(grads_, destination)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/comm.py\", line 142, in reduce_add_coalesced\n    flat_tensors = [_flatten_dense_tensors(chunk) for chunk in chunks]  # (num_gpus,)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/comm.py\", line 142, in <listcomp>\n    flat_tensors = [_flatten_dense_tensors(chunk) for chunk in chunks]  # (num_gpus,)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_utils.py\", line 459, in _flatten_dense_tensors\n    return torch._C._nn.flatten_dense_tensors(tensors)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.47 GiB already allocated; 11.75 MiB free; 13.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">radiant-sweep-5</strong> at: <a href='https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/ptd1z34q' target=\"_blank\">https://wandb.ai/import_this/llm_science_exam_open_book_approach/runs/ptd1z34q</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231209_000608-ptd1z34q/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run ptd1z34q errored: OutOfMemoryError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.47 GiB already allocated; 11.75 MiB free; 13.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ptd1z34q errored: OutOfMemoryError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.47 GiB already allocated; 11.75 MiB free; 13.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We are far behind the best MAP@3 metric of the competition, which is 0.933208 made by [Team H2O LLM Studio][1] but that is to be expected. They have used a dataset of 2.46 TB of data, where we only used our original 200 questions and answers with the additional context column. Nonetheless, it is exciting to see that our model reached a far better outcome than random guessing from such a small dataset.\n\n\nMAP@3 is the official evaluaion metric of the competition, but out of curiosity we checked the accuracy too.\n\n[1]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446422","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}